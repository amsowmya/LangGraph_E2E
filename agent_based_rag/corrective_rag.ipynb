{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2a28794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode, tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42394ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "llm.invoke(\"hi\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c3b1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c66ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_splitter = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splitter,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78274359",
   "metadata": {},
   "source": [
    "#### Let's create a RAG chain now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0acbe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" \n",
    "You are an assistant for question-answering task. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximun and keep the answer concise.\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d06c258f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of LLM-powered autonomous agents, memory refers to the processes used to acquire, store, retain, and retrieve information. There are two types of memory: short-term memory, which utilizes in-context learning and has a limited capacity, and long-term memory, which provides the capability to retain and recall information over extended periods using an external vector store and fast retrieval.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"tell me about agent memory\"\n",
    "rag_chain.invoke({\"context\": docs[0], \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eeb1c4",
   "metadata": {},
   "source": [
    "#### Now lets create grade document class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb6b343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    \n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdd85e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system_msg = \"\"\" You are a grader accessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to question, grade it as relevant. \\n\n",
    "Give binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_msg),\n",
    "        (\"user\", \"Retrieved document: \\n\\n{document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd85a5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sowmy\\AppData\\Local\\Temp\\ipykernel_14964\\3694875819.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "question = \"tell me about agent memory\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4e83109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "question = \"Who built taj Mahal\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9df404",
   "metadata": {},
   "source": [
    "#### Let's create Question Re-Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "723be6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"You are a question re-writer that converts an input question to better version that is optimized \\n\n",
    "for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_msg),\n",
    "        (\"user\", \n",
    "         \"Here is the initial question: \\n\\n {question} \\n Formulate an improved quesiton.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5c0a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s an improved version of the question:\\n\\n\"Who was the architect and builder of the Taj Mahal in India?\"\\n\\nThis revised question is more specific and provides more context, making it easier to find accurate and relevant information through a web search. \\n\\nAlternatively, you could also use:\\n\\n* \"Taj Mahal construction: who was the main architect and builder?\"\\n* \"History of the Taj Mahal: who built it and when?\"\\n* \"Who designed and constructed the Taj Mahal monument in Agra, India?\"\\n\\nThese rephrased questions can help you get more precise and informative results from your search.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d857c",
   "metadata": {},
   "source": [
    "#### Let's create required function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4defddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\" \n",
    "    Retrieve documents \n",
    "    \n",
    "    Args: \n",
    "        state (dict): The current graph state \n",
    "        \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    \n",
    "    question = state['question']\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7307905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
